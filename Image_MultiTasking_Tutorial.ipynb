{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from torchvision.io import read_image"
      ],
      "metadata": {
        "id": "UB3maNwt0roV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5_vXR0dT0r9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im1=read_image(\"000000005477.jpg\")"
      ],
      "metadata": {
        "id": "-hGZui_z0w97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import vit_h_14, ViT_H_14_Weights\n",
        "\n",
        "# Step 1: Initialize model with the best available weights\n",
        "weights = ViT_H_14_Weights.DEFAULT\n",
        "model = vit_h_14(weights='DEFAULT')\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0JeHh_m5qmK",
        "outputId": "4927f663-793d-4c4f-f67a-7eb7eb8365cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_h_14_swag-80465313.pth\" to /root/.cache/torch/hub/checkpoints/vit_h_14_swag-80465313.pth\n",
            "100%|██████████| 2.36G/2.36G [00:39<00:00, 63.4MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (conv_proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): Sequential(\n",
              "      (encoder_layer_0): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_1): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_2): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_3): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_4): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_5): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_6): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_7): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_8): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_9): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_10): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_11): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_12): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_13): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_14): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_15): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_16): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_17): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_18): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_19): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_20): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_21): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_22): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_23): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_24): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_25): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_26): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_27): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_28): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_29): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_30): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_31): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (heads): Sequential(\n",
              "    (head): Linear(in_features=1280, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th3lBuOHzu6e",
        "outputId": "f810a48c-955e-4309-ce7e-2c33138ad634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "airliner: 91.4%\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Initialize the inference transforms\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# Step 3: Apply inference preprocessing transforms\n",
        "batch = preprocess(im1).unsqueeze(0)\n",
        "\n",
        "# Step 4: Use the model and print the predicted category\n",
        "prediction = model(batch).squeeze(0).softmax(0)\n",
        "class_id = prediction.argmax().item()\n",
        "score = prediction[class_id].item()\n",
        "category_name = weights.meta[\"categories\"][class_id]\n",
        "print(f\"{category_name}: {100 * score:.1f}%\") #airliner: ~91.4%"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vit_h_14, ViT_H_14_Weights\n",
        "from transformers import ViTFeatureExtractor"
      ],
      "metadata": {
        "id": "Xk42FRJb3M_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained ViT model\n",
        "weights = ViT_H_14_Weights.DEFAULT\n",
        "vit_model = vit_h_14(weights='DEFAULT') # Downloading: \"https://download.pytorch.org/models/vit_h_14_swag-80465313.pth\" to /root/.cache/torch/hub/checkpoints/vit_h_14_swag-80465313.pth\n",
        "model = vit_model\n",
        "vit_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGqXpeek4LJ-",
        "outputId": "63c4e9ef-ee16-431c-f822-e187ebcbafb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_h_14_swag-80465313.pth\" to /root/.cache/torch/hub/checkpoints/vit_h_14_swag-80465313.pth\n",
            "100%|██████████| 2.36G/2.36G [00:10<00:00, 235MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (conv_proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): Sequential(\n",
              "      (encoder_layer_0): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_1): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_2): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_3): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_4): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_5): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_6): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_7): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_8): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_9): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_10): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_11): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_12): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_13): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_14): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_15): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_16): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_17): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_18): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_19): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_20): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_21): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_22): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_23): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_24): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_25): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_26): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_27): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_28): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_29): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_30): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_31): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (heads): Sequential(\n",
              "    (head): Linear(in_features=1280, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: to.device(CUDA) needed for GPU resources\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"CUDA is available. Using GPU.\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"CUDA is not available. Using CPU.\")\n",
        "\n",
        "model = model.to(device) # can be replaced with segmentation_model\n",
        "batch = batch.to(device)"
      ],
      "metadata": {
        "id": "UxAHR-ZEC61K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model = model\n",
        "num_classes = 21"
      ],
      "metadata": {
        "id": "SFGjflgoiluH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import ViT_H_14_Weights, vit_h_14\n",
        "\n",
        "class ViTSegmentation(nn.Module):\n",
        "    def __init__(self, vit_model, num_classes):\n",
        "        super(ViTSegmentation, self).__init__()\n",
        "        self.vit = vit_model\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1000, 512, kernel_size=2, stride=2), # Added transposed convolution layers\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, num_classes, kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.vit(x)\n",
        "        features = features.unsqueeze(2).unsqueeze(3)\n",
        "        features = features.expand(-1, 1000, 14, 14)\n",
        "        output = self.decoder(features)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Initialize the segmentation model with the pretrained ViT\n",
        "num_classes = 21  # Number of output classes for segmentation\n",
        "segmentation_model = ViTSegmentation(vit_model=vit_model, num_classes=21)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "segmentation_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ7VDI-smyeD",
        "outputId": "f15e98d7-bca9-46a4-e609-2bc50dc0dee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTSegmentation(\n",
              "  (vit): VisionTransformer(\n",
              "    (conv_proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
              "    (encoder): Encoder(\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "      (layers): Sequential(\n",
              "        (encoder_layer_0): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_1): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_2): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_3): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_4): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_5): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_6): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_7): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_8): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_9): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_10): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_11): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_12): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_13): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_14): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_15): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_16): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_17): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_18): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_19): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_20): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_21): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_22): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_23): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_24): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_25): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_26): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_27): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_28): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_29): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_30): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (encoder_layer_31): EncoderBlock(\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (4): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "    )\n",
              "    (heads): Sequential(\n",
              "      (head): Linear(in_features=1280, out_features=1000, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): ConvTranspose2d(1000, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (3): ReLU()\n",
              "    (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (5): ReLU()\n",
              "    (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (7): ReLU()\n",
              "    (8): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (9): ReLU()\n",
              "    (10): ConvTranspose2d(32, 21, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input\n",
        "input_tensor = torch.randn(1, 3, 518, 518)\n",
        "\n",
        "# Forward pass\n",
        "output = segmentation_model(input_tensor)\n",
        "\n",
        "print(output.shape)  # Should be [1, 21, 518, 518]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-YsJkKwlXbu",
        "outputId": "e8211c2f-ffe7-481a-b074-541d9bf03e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 21, 896, 896])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image_path = '/content/drive/MyDrive/Cityscapes/test/berlin_000000_000019_leftImg8bit.png'  # Replace with your image path"
      ],
      "metadata": {
        "id": "1SJNgUjMoUlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "# Example usage\n",
        "image_path = '000000005477.jpg'  # Replace with your image path\n",
        "image = Image.open(image_path).convert(\"RGB\")  # Ensure image is in RGB format"
      ],
      "metadata": {
        "id": "KHghEXGI_Ulp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "2w1Sg1xO_rGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Initialize the inference transforms\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# Step 3: Apply inference preprocessing transforms\n",
        "batch = preprocess(image).unsqueeze(0)\n",
        "\n",
        "# Step 4: Use the model and print the predicted category\n",
        "prediction = model(batch).squeeze(0).softmax(0)\n",
        "class_id = prediction.argmax().item()\n",
        "score = prediction[class_id].item()\n",
        "category_name = weights.meta[\"categories\"][class_id]\n",
        "print(f\"{category_name}: {100 * score:.1f}%\") # traffic light: 63.0%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtPUhniVA_Sz",
        "outputId": "37a25b41-46b9-4609-f440-c2d555aa0258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "traffic light: 63.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already defined ViTSegmentation as before\n",
        "# Load the pretrained ViT model and your custom segmentation model\n",
        "# segmentation_model = ViTSegmentation(vit_model=vit_model, num_classes=21)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "# segmentation_model.eval()\n",
        "\n",
        "# Define a function to preprocess an existing image tensor\n",
        "def preprocess_image_tensor(image):\n",
        "    # Define the transformations (resize, normalization) similar to ViTFeatureExtractor\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((518, 518)),  # Resize to match ViT input size\n",
        "        transforms.ConvertImageDtype(torch.float),  # Convert to float32\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
        "    ])\n",
        "\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    # Add batch dimension\n",
        "    image_tensor = image_tensor.unsqueeze(0)  # Shape: [1, 3, 224, 224]\n",
        "\n",
        "    return image_tensor\n",
        "\n",
        "# Define a function to postprocess the output\n",
        "def postprocess_output(output):\n",
        "    # Apply argmax to get the most likely class for each pixel\n",
        "    output_predictions = torch.argmax(output, dim=1).squeeze().detach().cpu().numpy()\n",
        "\n",
        "    return output_predictions\n",
        "\n",
        "# Read the image\n",
        "image_tensor = read_image(image_path)  # This reads image as [C, H, W]\n",
        "\n",
        "# Ensure the image has 3 channels\n",
        "if image_tensor.shape[0] != 3:\n",
        "    raise ValueError(f\"Expected 3 channels, but got {image_tensor.shape[0]} channels\")\n",
        "\n",
        "# Preprocess the image tensor\n",
        "input_tensor = preprocess_image_tensor(image_tensor)"
      ],
      "metadata": {
        "id": "pkuqm0b--RiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06iQnx1BBreP",
        "outputId": "587c1881-eb80-4b93-b9f8-28936e1be880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 518, 518])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"CUDA is available. Using GPU.\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"CUDA is not available. Using CPU.\")\n",
        "\n",
        "model = segmentation_model.to(device)\n",
        "batch = batch.to(device)"
      ],
      "metadata": {
        "id": "n3rYLukaGIJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    output = segmentation_model(input_tensor)"
      ],
      "metadata": {
        "id": "gfWgNCcD-0R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Postprocess the output\n",
        "segmentation_map = postprocess_output(output)\n",
        "\n",
        "# Convert to an image for visualization\n",
        "segmentation_image = Image.fromarray(segmentation_map.astype(np.uint8))\n",
        "\n",
        "# Save or display the segmentation image\n",
        "segmentation_image.save('***/Cityscapes/segmentation_map.png')\n",
        "segmentation_image.show()"
      ],
      "metadata": {
        "id": "Oy1TQZyYpMjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}